{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Crystalheart0828/AI-projects-notebooks/blob/main/Content_Crawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ6h4xEoYJnD"
      },
      "source": [
        "# Content Crawler\n",
        "\n",
        "Created with heart and soul by [Karen Ding](https://karending.com)<br>\n",
        "Last updated on Mar 2, 2025\n",
        "\n",
        "\n",
        "## License and Usage\n",
        "This tutorial is licensed under the MIT License.\n",
        "\n",
        "Feel free to use, modify, and share this tutorial!\n",
        "\n",
        "If you find it helpful, please consider:\n",
        "\n",
        "1. Linking back to the [original webpage](https://karending.com)\n",
        "2. Mentioning where you learned from it\n",
        "3. Contributing improvements back to help others learn too\n",
        "\n",
        "You are free to:\n",
        "\n",
        "1. Use and adapt this tutorial for any purpose.\n",
        "2. Share, copy, and redistribute it in any medium.\n",
        "3. Modify, remix, and build upon the material.\n",
        "</br>\n",
        "\n",
        "For the full license details, please see the [LICENSE](https://github.com/Crystalheart0828/AI-projects-notebooks/blob/main/LICENSE) file included with this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV7yctxKtwdR"
      },
      "source": [
        "## Overview\n",
        "\n",
        "**Content Crawler** is a seamless web scraping tool that lets you input URLs into a designated column, automatically extracting web page content via a Python script and exporting it to Google Docs, securely stored in your Google Drive.\n",
        "\n",
        "This tutorial will show you how to build a content scraper that automatically:\n",
        "1. Reads URLs from a Google Sheet\n",
        "2. Extracts content from each webpage\n",
        "3. Saves the content to Google Docs\n",
        "4. Organizes everything in Google Drive folders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAmBr1CJtwdS"
      },
      "source": [
        "## Prerequisites (Your Key Ingredients! üß∞)\n",
        "\n",
        "#### **Google Cloud Credentials, and APIs (Google Sheet, Google Drive, Google Doc)**\n",
        "Don't know what this is? No worries‚ÄîI've been there too! Simply ask any LLM (e.g., ChatGPT, Claude, Gemini, etc.) the following prompt:\n",
        "\n",
        "> I'm new to Google Cloud Console and haven't built a project yet. Could you please walk me through the step-by-step process to download the credentials file and activate the Google Sheets API, Google Drive API, and Google Docs API?\n",
        "\n",
        "#### **Google Sheet with URLs**\n",
        "- Create a Google Sheet\n",
        "- Put your URLs in Column A\n",
        "- Share the sheet with your service account email\n",
        "- Copy the Sheet ID from the URL (the long string between /d/ and /edit)\n",
        "\n",
        "#### **Google Drive Folder**\n",
        "- Create a folder in Google Drive\n",
        "- Share it with your service account email\n",
        "- Copy the Folder ID from the URL\n",
        "\n",
        "#### **Google Colab**\n",
        "[Google Colab](https://colab.research.google.com/) is a fantastic cloud-based IDE. Like Google Docs, your scripts are saved in your Google Drive and can be easily shared with others. Simply sign in to Google Colab with your Google account, and you're ready to get started."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nivqBnlytwdS"
      },
      "source": [
        "## Step-by-Step Guide"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOQP-ixGtwdT"
      },
      "source": [
        "### **Step 1: Setting Up Your Environment üõ†Ô∏è**\n",
        "**What's happening here**: Think of this as preparing your kitchen before cooking. We're getting all our tools ready by installing the necessary Python packages. Just like you can't cook without pots and pans, you can't run the script without these packages!\n",
        "\n",
        "We are going to install the following Python packages:\n",
        "* google-auth (for Google authentication)\n",
        "* google-api-python-client (for using Google services)\n",
        "* gspread (for working with Google Sheets)\n",
        "* beautifulsoup4 (for parsing web content)\n",
        "* requests (for making internet requests)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJcaph3rtwdT"
      },
      "outputs": [],
      "source": [
        "pip install google-auth google-api-python-client gspread beautifulsoup4 requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCfLj-6RtwdU"
      },
      "source": [
        "### **Step 2: Importing Required Tools üìö**\n",
        "**What's happening here**: Now we're bringing in all the tools we'll need. It's like taking out all the utensils and ingredients from your kitchen cabinets before starting to cook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioycKK9CtwdV"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import gspread\n",
        "from google.oauth2.service_account import Credentials\n",
        "from googleapiclient.discovery import build\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from datetime import datetime\n",
        "import time\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpahA7lZtwdV"
      },
      "source": [
        "### **Step 3: Setting Up Google Authentication üîë**\n",
        "**What's happening here**: This is like showing your ID to enter a restricted area. We're telling Google who we are and what we want to do with their services (in this case, use Google Sheets, Drive, and Docs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEE2vpSHtwdV"
      },
      "source": [
        "#### Setting up Colab Secrets\n",
        "\n",
        "Before running any code, you need to set up your secrets in Colab:\n",
        "\n",
        "1. Click on the folder icon on the left sidebar\n",
        "2. Click on the key icon (üîë) to open the Secrets panel\n",
        "3. Add the following secrets:\n",
        "   - `service_account_json`: Paste the entire contents of your service account JSON file\n",
        "   - `sheet_id`: Your Google Sheet ID\n",
        "   - `folder_id`: Your Google Drive folder ID\n",
        "   - `user_email`: Your email address"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpt9GDtItwdW"
      },
      "outputs": [],
      "source": [
        "# Define what permissions we need\n",
        "SCOPES = [\n",
        "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
        "    \"https://www.googleapis.com/auth/drive\",\n",
        "    \"https://www.googleapis.com/auth/documents\"\n",
        "]\n",
        "\n",
        "credentials_path ='your_credentials_file.json' # Replace with your actual credentials file path\n",
        "\n",
        "\n",
        "# Get our configuration from Colab secrets\n",
        "service_account_info = json.load(open(credentials_path))\n",
        "SHEET_ID = userdata.get('sheet_id')\n",
        "PARENT_FOLDER_ID = userdata.get('folder_id')\n",
        "USER_EMAIL = userdata.get('user_email')\n",
        "SHEET_NAME = \"Sheet1\"  # Change this if your sheet has a different name\n",
        "\n",
        "# Authenticate with Google\n",
        "creds = Credentials.from_service_account_info(service_account_info, scopes=SCOPES)\n",
        "client = gspread.authorize(creds)\n",
        "drive_service = build('drive', 'v3', credentials=creds)\n",
        "docs_service = build('docs', 'v1', credentials=creds)\n",
        "\n",
        "print(\"Successfully connected to Google services!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Get Your Service Account Email:"
      ],
      "metadata": {
        "id": "qNgHTUnhMFbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "service_account_email = service_account_info['client_email']\n",
        "print(f\"Your service account email is: {service_account_email}\")"
      ],
      "metadata": {
        "id": "2uCG_6wqLtTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Share Your Google Sheet:\n",
        "* Open your Google Sheet in your browser\n",
        "* Click the \"Share\" button in the top right corner\n",
        "* In the \"Share with people and groups\" field, paste your service account email\n",
        "* Set the role to \"Editor\"\n",
        "* Click \"Send\" (Note: You don't need to check the \"Notify people\" box)"
      ],
      "metadata": {
        "id": "0zk1Vc4WMJ3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üí° **Pro Tips!**\n",
        "\n",
        "* The service account email usually ends with **\"@*.iam.gserviceaccount.com\"**\n",
        "* Make sure to give **\"Editor\"** access if you want the script to write to the sheet.</br>\n",
        "* You only need to do this sharing step once for each Google Sheet"
      ],
      "metadata": {
        "id": "dT1iia5WMWuk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bko4jaCstwdW"
      },
      "source": [
        "### **Step 4: Creating Helper Functions üîß**\n",
        "What's happening here: We're creating specialized tools for each task. Think of these as your cooking techniques - each one has a specific purpose in preparing our final dish."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### a. Extract urls function\n",
        "Retrieves a list of URLs from column F of a specified Google Sheet, skipping the header row."
      ],
      "metadata": {
        "id": "FWWlOuphM1RC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_urls_from_sheet(sheet_id, sheet_name):\n",
        "    \"\"\"Gets all the URLs from our Google Sheet\"\"\"\n",
        "    try:\n",
        "        sheet = client.open_by_key(sheet_id)\n",
        "        worksheet = sheet.worksheet(sheet_name)\n",
        "        urls = worksheet.col_values(6)[1:]  # Get URLs from column F, skip header\n",
        "        return [url for url in urls if url.strip()]\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing sheet: {str(e)}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "1IKevdeSMhGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### b. Scrape content function\n",
        "Fetches and extracts readable text content from a given webpage, and returns it along with the original URL."
      ],
      "metadata": {
        "id": "VJ4KboMXNBC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_content_from_url(url):\n",
        "    \"\"\"Reads the content from each webpage\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        response.encoding = 'utf-8'\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        paragraphs = soup.find_all('p')\n",
        "        if not paragraphs:\n",
        "            paragraphs = soup.find_all('div')\n",
        "\n",
        "        article_content = ' '.join([p.get_text() for p in paragraphs])\n",
        "\n",
        "        if len(article_content) < 50:\n",
        "            print(f\"Warning: Content for {url} is very short or empty.\")\n",
        "            return None\n",
        "\n",
        "        return f\"Original URL: {url}\\n\\n{article_content}\"\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Failed to retrieve content from {url}: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "ofB1CI-QMkTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### c. Create folder in Google Drive function\n",
        "Creates a new folder in Google Drive under a specified parent folder, or returns the existing folder's ID if it already exists."
      ],
      "metadata": {
        "id": "aK3P8oeLNG7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_folder_in_drive(folder_name, parent_folder_id):\n",
        "    \"\"\"Creates a new folder in Google Drive\"\"\"\n",
        "    query = f\"'{parent_folder_id}' in parents and mimeType='application/vnd.google-apps.folder' and name='{folder_name}'\"\n",
        "    results = drive_service.files().list(q=query).execute()\n",
        "    folders = results.get('files', [])\n",
        "\n",
        "    if folders:\n",
        "        return folders[0]['id']\n",
        "    else:\n",
        "        file_metadata = {\n",
        "            'name': folder_name,\n",
        "            'mimeType': 'application/vnd.google-apps.folder',\n",
        "            'parents': [parent_folder_id]\n",
        "        }\n",
        "        folder = drive_service.files().create(body=file_metadata, fields='id').execute()\n",
        "        return folder['id']"
      ],
      "metadata": {
        "id": "U7JrQiqqMnID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### d. Export the scraped content to Google Doc function\n",
        "Creates a new Google Doc with the given content and title, moves it to a specified Google Drive folder, and shares it with a user via email."
      ],
      "metadata": {
        "id": "vizY_jNYNOis"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rry0wkqgtwdW"
      },
      "outputs": [],
      "source": [
        "def create_and_share_doc(content, title, folder_id):\n",
        "    \"\"\"Creates a new Google Doc and shares it\"\"\"\n",
        "    # Create the doc\n",
        "    doc = docs_service.documents().create(body={\"title\": title}).execute()\n",
        "    document_id = doc['documentId']\n",
        "\n",
        "    # Add content\n",
        "    requests_body = [{\n",
        "        'insertText': {\n",
        "            'location': {'index': 1},\n",
        "            'text': content\n",
        "        }\n",
        "    }]\n",
        "    docs_service.documents().batchUpdate(\n",
        "        documentId=document_id,\n",
        "        body={'requests': requests_body}\n",
        "    ).execute()\n",
        "\n",
        "    # Move to folder\n",
        "    drive_service.files().update(\n",
        "        fileId=document_id,\n",
        "        addParents=folder_id,\n",
        "        fields='id, parents'\n",
        "    ).execute()\n",
        "\n",
        "    # Share the doc\n",
        "    drive_service.permissions().create(\n",
        "        fileId=document_id,\n",
        "        body={\n",
        "            'type': 'user',\n",
        "            'role': 'writer',\n",
        "            'emailAddress': USER_EMAIL\n",
        "        },\n",
        "        fields='id',\n",
        "        sendNotificationEmail=True\n",
        "    ).execute()\n",
        "\n",
        "    return document_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akp1wXDttwdX"
      },
      "source": [
        "### **Step 5: Let's Start Scraping! üöÄ**\n",
        "\n",
        "Now for the fun part - let's put everything together and start scraping! This cell will:\n",
        "1. Create a new folder for today's content\n",
        "2. Get all URLs from your sheet\n",
        "3. Visit each webpage and save its content\n",
        "4. Create a Google Doc for each page\n",
        "5. Share everything with you"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AG9ssdb1twdY"
      },
      "outputs": [],
      "source": [
        "# Create today's folder\n",
        "current_date = datetime.now().strftime(\"%Y%m%d\")\n",
        "session_folder_name = f\"Scraped_Content_{current_date}\"\n",
        "folder_id = create_folder_in_drive(session_folder_name, PARENT_FOLDER_ID)\n",
        "print(f\"Created session folder: {session_folder_name}\")\n",
        "\n",
        "# Create Original content folder\n",
        "original_folder_id = create_folder_in_drive(\"Original\", folder_id)\n",
        "print(\"Created 'Original' folder for raw content\")\n",
        "\n",
        "# Get URLs from sheet\n",
        "urls = extract_urls_from_sheet(SHEET_ID, SHEET_NAME)\n",
        "print(f\"Found {len(urls)} URLs to process\")\n",
        "\n",
        "# Process each URL\n",
        "for i, url in enumerate(urls, start=1):\n",
        "    print(f\"\\nProcessing URL {i}/{len(urls)}: {url}\")\n",
        "\n",
        "    content = scrape_content_from_url(url)\n",
        "    if content:\n",
        "        # Create and share the document\n",
        "        doc_title = f\"Original-{i}-{current_date}\"\n",
        "        document_id = create_and_share_doc(content, doc_title, original_folder_id)\n",
        "        print(f\"Created and shared document: https://docs.google.com/document/d/{document_id}/edit\")\n",
        "\n",
        "        # Take a short break\n",
        "        time.sleep(1)\n",
        "    else:\n",
        "        print(f\"Skipped URL {url} due to scraping failure\")\n",
        "\n",
        "print(\"\\nAll done! Check your email for the shared documents.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUZ7fO9StwdY"
      },
      "source": [
        "#### üí° **Pro Tips:**\n",
        "1. Your Colab secrets are secure and won't be visible in your notebook's output or shared versions\n",
        "2. Remember to set up your secrets before running the notebook\n",
        "3. If you're sharing your notebook, the secrets won't be shared - each user needs to set up their own\n",
        "4. Colab sessions expire after a while, so you might need to rerun the authentication steps\n",
        "5. To avoid timeouts during long scraping sessions, you can go to Runtime ‚Üí Change runtime type and select a GPU or TPU runtime for better performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THllj5dKtwdY"
      },
      "source": [
        "¬© 2025 Karen Ding. Created with heart and soul.\n",
        "</br>Licensed under the MIT License. See [LICENSE](https://github.com/Crystalheart0828/AI-projects-notebooks/blob/main/LICENSE) file for details.\n",
        "</br>\n",
        "Find more tutorials at [karending.com](https://karending.com)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}