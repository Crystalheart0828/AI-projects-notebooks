{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Automate Market Research\n"
      ],
      "metadata": {
        "id": "tZ6h4xEoYJnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This AI tool is\n",
        "\n",
        "### Environment: Googloe Colab,"
      ],
      "metadata": {
        "id": "XUYxg6AgSGsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  啟動前，先在secret keys選項中變更`episode_folder_name`, `starting_date`, `ending_date`\n",
        "2.  先啟動上半部分>>挑選好主題 >>啟動下半部分\n"
      ],
      "metadata": {
        "id": "CK27XYgiGXlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install newsapi-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE_b9yevYRU_",
        "outputId": "3ac86bed-d3f5-4b9a-ab72-1daee485ee7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newsapi-python\n",
            "  Downloading newsapi_python-0.2.7-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.10/dist-packages (from newsapi-python) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0->newsapi-python) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0->newsapi-python) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0->newsapi-python) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0->newsapi-python) (2024.12.14)\n",
            "Downloading newsapi_python-0.2.7-py2.py3-none-any.whl (7.9 kB)\n",
            "Installing collected packages: newsapi-python\n",
            "Successfully installed newsapi-python-0.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gspread\n",
        "from google.oauth2.service_account import Credentials\n",
        "from newsapi import NewsApiClient\n",
        "import json\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from googleapiclient.discovery import build\n",
        "from datetime import datetime\n",
        "\n",
        "import time\n",
        "\n",
        "from google.colab import userdata\n",
        "import os"
      ],
      "metadata": {
        "id": "h87MI-jJYXm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the scope and authorize the credentials\n",
        "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\", \"https://www.googleapis.com/auth/drive\"]\n",
        "credentials_path = '/content/flying-chicken-433105-b571fb548d0f.json'\n",
        "creds = Credentials.from_service_account_file(credentials_path, scopes=SCOPES)"
      ],
      "metadata": {
        "id": "Vyx0W6kcYYvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize gspread client\n",
        "client = gspread.authorize(creds)"
      ],
      "metadata": {
        "id": "D8kzPPfAYapI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Collect News"
      ],
      "metadata": {
        "id": "qZZVatcreoKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect NewsAPI"
      ],
      "metadata": {
        "id": "wWORvUb_YhDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Init\n",
        "newsapi = NewsApiClient(api_key = userdata.get('news_api'))"
      ],
      "metadata": {
        "id": "0z7EAucwYbLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Functions"
      ],
      "metadata": {
        "id": "FRi3TWrKZ5zt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Export to google sheet"
      ],
      "metadata": {
        "id": "AlYWnZCJZ73k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def export_to_google_sheet(all_articles, sheet_id, sheet_name):\n",
        "    # Open the Google Sheet and add a new worksheet or access an existing one\n",
        "    sheet = client.open_by_key(sheet_id)\n",
        "    try:\n",
        "        worksheet = sheet.worksheet(sheet_name)\n",
        "        worksheet.clear()\n",
        "    except gspread.exceptions.WorksheetNotFound:\n",
        "        worksheet = sheet.add_worksheet(title=sheet_name, rows=\"100\", cols=\"20\")\n",
        "\n",
        "    # Prepare the header\n",
        "    header = [\"Title\", \"Author\", \"Source\", \"Published At\", \"Description\", \"URL\", \"Content\"]\n",
        "    worksheet.append_row(header)\n",
        "\n",
        "    # Prepare the data rows for batch update\n",
        "    rows = []\n",
        "    for article in all_articles['articles']:\n",
        "        title = article.get('title', 'No Title')\n",
        "        author = article.get('author', 'No Author')\n",
        "        source = article['source']['name']\n",
        "        published_at = article.get('publishedAt')\n",
        "        description = article.get('description', 'No Description')\n",
        "        url = article.get('url', 'No URL')\n",
        "        content = article.get('content', 'No Content')\n",
        "\n",
        "        rows.append([title, author, source, published_at, description, url, content])\n",
        "\n",
        "    # Perform batch update\n",
        "    if rows:\n",
        "        worksheet.append_rows(rows)\n",
        "\n",
        "    print(f\"Articles exported successfully to the sheet: https://docs.google.com/spreadsheets/d/{sheet_id}/edit#gid={worksheet.id}\")\n"
      ],
      "metadata": {
        "id": "Ph_WN8SGZ47W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query News API"
      ],
      "metadata": {
        "id": "kY0WYFbzaI93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to query NewsAPI for a given keyword\n",
        "def query_news_api(keyword, starting_date, ending_date, pages):\n",
        "    all_articles = newsapi.get_everything(q=keyword,\n",
        "                                          sources=None,\n",
        "                                          domains=None,\n",
        "                                          from_param= starting_date,\n",
        "                                          to= ending_date,\n",
        "                                          language='en',\n",
        "                                          sort_by='relevancy',\n",
        "                                          page= pages)\n",
        "    return all_articles"
      ],
      "metadata": {
        "id": "nAWAZmdRYcJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process multiple keywords"
      ],
      "metadata": {
        "id": "8ubi1buebHkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_keywords(keywords, starting_date, ending_date, pages, sheet_id):\n",
        "    for keyword in keywords:\n",
        "        print(f\"Processing keyword: {keyword}\")\n",
        "        # Query the News API for the current keyword\n",
        "        all_articles = query_news_api(keyword, starting_date, ending_date, pages)\n",
        "\n",
        "        # Use the keyword directly as the sheet name\n",
        "        sheet_name = keyword  # Assuming the tab in the Google Sheet has the exact same name as the keyword\n",
        "\n",
        "        # Export the articles to the specific sheet/tab named after the keyword\n",
        "        export_to_google_sheet(all_articles, sheet_id, sheet_name)\n",
        "        print(f\"Finished processing for keyword: {keyword}\")"
      ],
      "metadata": {
        "id": "SFSJ9VPfYce0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Main Function"
      ],
      "metadata": {
        "id": "jhGOZM7jbL8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    keywords = [\"AI and marketing\",\"AI and media\", \"AI and innovation\", \"AI and business\", \"AI and management\"]\n",
        "    starting_date = userdata.get('starting_date')\n",
        "    ending_date = userdata.get('ending_date')\n",
        "    pages = 5\n",
        "    sheet_id = userdata.get('sheet_id')\n",
        "\n",
        "    process_keywords(keywords, starting_date, ending_date, pages, sheet_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmCq855bYcuX",
        "outputId": "2ce8f1d0-b6ea-4cc7-d133-db1e09f9181c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing keyword: AI and marketing\n",
            "Articles exported successfully to the sheet: https://docs.google.com/spreadsheets/d/1pvhylBx6ixPXGNs_-66gedMt-V7a1ggETSe44ztCAug/edit#gid=1695063662\n",
            "Finished processing for keyword: AI and marketing\n",
            "Processing keyword: AI and media\n",
            "Articles exported successfully to the sheet: https://docs.google.com/spreadsheets/d/1pvhylBx6ixPXGNs_-66gedMt-V7a1ggETSe44ztCAug/edit#gid=499554999\n",
            "Finished processing for keyword: AI and media\n",
            "Processing keyword: AI and innovation\n",
            "Articles exported successfully to the sheet: https://docs.google.com/spreadsheets/d/1pvhylBx6ixPXGNs_-66gedMt-V7a1ggETSe44ztCAug/edit#gid=368577039\n",
            "Finished processing for keyword: AI and innovation\n",
            "Processing keyword: AI and business\n",
            "Articles exported successfully to the sheet: https://docs.google.com/spreadsheets/d/1pvhylBx6ixPXGNs_-66gedMt-V7a1ggETSe44ztCAug/edit#gid=978405254\n",
            "Finished processing for keyword: AI and business\n",
            "Processing keyword: AI and management\n",
            "Articles exported successfully to the sheet: https://docs.google.com/spreadsheets/d/1pvhylBx6ixPXGNs_-66gedMt-V7a1ggETSe44ztCAug/edit#gid=608425818\n",
            "Finished processing for keyword: AI and management\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Scrape Content"
      ],
      "metadata": {
        "id": "4gcCgIwEetl7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract and open the filtered URLs"
      ],
      "metadata": {
        "id": "FoUEosj-qtrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract URLs from a Google Sheet\n",
        "def extract_urls_from_sheet(sheet_id, sheet_name):\n",
        "    sheet = client.open_by_key(sheet_id)\n",
        "\n",
        "    # Print all available sheet names for debugging\n",
        "    worksheets = sheet.worksheets()\n",
        "    available_sheet_names = [ws.title for ws in worksheets]\n",
        "    print(f\"Available sheet names: {available_sheet_names}\")\n",
        "\n",
        "    # Strip whitespace from the sheet name before accessing\n",
        "    sheet_name = sheet_name.strip()\n",
        "\n",
        "    if sheet_name not in available_sheet_names:\n",
        "        raise ValueError(f\"Sheet name '{sheet_name}' not found. Available sheet names: {available_sheet_names}\")\n",
        "\n",
        "    worksheet = sheet.worksheet(sheet_name)\n",
        "\n",
        "    # The URLs are in a specific column, e.g., column F (index 6)\n",
        "    urls = worksheet.col_values(6)[1:]  # Skip the header\n",
        "    return urls"
      ],
      "metadata": {
        "id": "fMbld4a8qswO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scrape web content"
      ],
      "metadata": {
        "id": "YkgiCTbluUug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scrape content from a URL and include the URL at the top\n",
        "def scrape_content_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()  # Raise an error for bad status codes\n",
        "        response.encoding = 'utf-8'  # Ensure proper encoding\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Extract main content\n",
        "        paragraphs = soup.find_all('p')\n",
        "        if not paragraphs:\n",
        "            # Fallback if no <p> tags found\n",
        "            paragraphs = soup.find_all('div')\n",
        "        article_content = ' '.join([p.get_text() for p in paragraphs])\n",
        "\n",
        "        # Check if content is minimal\n",
        "        if len(article_content) < 50:\n",
        "            print(f\"Warning: Content for {url} is very short or empty.\")\n",
        "            return None\n",
        "\n",
        "        # Prepend the URL to the article content\n",
        "        formatted_content = f\"Original URL: {url}\\n\\n{article_content}\"\n",
        "\n",
        "        return formatted_content\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Failed to retrieve content from {url}: {str(e)}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "YQbDHfXfqznu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a folder in Google Drive under a specified parent folder\n",
        "def create_folder_in_drive(drive_service, folder_name, parent_folder_id):\n",
        "    # Check if the folder already exists\n",
        "    query = f\"'{parent_folder_id}' in parents and mimeType='application/vnd.google-apps.folder' and name='{folder_name}'\"\n",
        "    results = drive_service.files().list(q=query).execute()\n",
        "    folders = results.get('files', [])\n",
        "\n",
        "    if folders:\n",
        "        # Folder already exists, return its ID\n",
        "        return folders[0]['id']\n",
        "    else:\n",
        "        # Create a new folder\n",
        "        file_metadata = {\n",
        "            'name': folder_name,\n",
        "            'mimeType': 'application/vnd.google-apps.folder',\n",
        "            'parents': [parent_folder_id]\n",
        "        }\n",
        "        folder = drive_service.files().create(body=file_metadata, fields='id').execute()\n",
        "        return folder['id']\n"
      ],
      "metadata": {
        "id": "k_U6lRoTk9Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Google Doc inside a specified folder\n",
        "def create_google_doc_in_folder(content, title, folder_id):\n",
        "    docs_service = build('docs', 'v1', credentials=creds)\n",
        "    drive_service = build('drive', 'v3', credentials=creds)\n",
        "\n",
        "    # Create a new Google Doc\n",
        "    doc = docs_service.documents().create(body={\"title\": title}).execute()\n",
        "    document_id = doc['documentId']\n",
        "\n",
        "    # Add content to the Google Doc\n",
        "    requests_body = [\n",
        "        {\n",
        "            'insertText': {\n",
        "                'location': {\n",
        "                    'index': 1,\n",
        "                },\n",
        "                'text': content\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "    docs_service.documents().batchUpdate(documentId=document_id, body={'requests': requests_body}).execute()\n",
        "\n",
        "    # Move the document to the specified folder\n",
        "    drive_service.files().update(fileId=document_id, addParents=folder_id, fields='id, parents').execute()\n",
        "\n",
        "    return document_id"
      ],
      "metadata": {
        "id": "s8bkAOxRlC7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Share a Google Doc with a specific email address\n",
        "def share_google_doc(document_id, email):\n",
        "    drive_service = build('drive', 'v3', credentials=creds)\n",
        "\n",
        "    user_permission = {\n",
        "        'type': 'user',\n",
        "        'role': 'writer',\n",
        "        'emailAddress': email\n",
        "    }\n",
        "    drive_service.permissions().create(\n",
        "        fileId=document_id,\n",
        "        body=user_permission,\n",
        "        fields='id',\n",
        "        sendNotificationEmail=True  # This will send a notification email to the user\n",
        "    ).execute()\n"
      ],
      "metadata": {
        "id": "RjgIGrs6q487"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to process URLs and export each to an individual Google Doc\n",
        "def process_urls_and_export_to_docs(sheet_id, sheet_names, user_email, parent_folder_id):\n",
        "    current_date = datetime.now().strftime(\"%Y%m%d\")\n",
        "\n",
        "    # Authenticate and build the Google Drive API service\n",
        "    drive_service = build('drive', 'v3', credentials=creds)\n",
        "\n",
        "    # Create the \"Episode 8\" folder under the given parent folder\n",
        "    episode_folder_name = userdata.get('episode_folder_name')\n",
        "    folder_id = create_folder_in_drive(drive_service, episode_folder_name, parent_folder_id)\n",
        "    print(f\"Folder '{episode_folder_name}' created with ID: {folder_id}\")\n",
        "\n",
        "    # Create the \"Original\" folder under the \"Episode 8\" folder\n",
        "    original_folder_name = \"Original\"\n",
        "    original_folder_id = create_folder_in_drive(drive_service, original_folder_name, folder_id)\n",
        "    print(f\"Folder '{original_folder_name}' created with ID: {original_folder_id}\")\n",
        "\n",
        "    # Ensure sheet_names is a list\n",
        "    if isinstance(sheet_names, str):\n",
        "        sheet_names = [sheet_names]  # Convert string to list with one item\n",
        "\n",
        "    # Loop through the sheet names to extract URLs and create docs\n",
        "    for sheet_name in sheet_names:\n",
        "        print(f\"Processing sheet: {sheet_name}\")\n",
        "\n",
        "        # Extract URLs from the current sheet\n",
        "        urls = extract_urls_from_sheet(sheet_id, sheet_name)\n",
        "\n",
        "        for i, url in enumerate(urls, start=1):\n",
        "            print(f\"Processing URL {i}: {url}\")\n",
        "            content = scrape_content_from_url(url)\n",
        "            if content:\n",
        "                # Create Google Doc with \"Original\" prefix\n",
        "                doc_title = f\"Original- {i} - {current_date}\"\n",
        "                document_id = create_google_doc_in_folder(content, title=doc_title, folder_id=original_folder_id)\n",
        "                share_google_doc(document_id, user_email)\n",
        "                print(f\"Document {doc_title} created and shared: https://docs.google.com/document/d/{document_id}/edit\")\n",
        "                time.sleep(1)  # Delay to avoid overwhelming the server\n",
        "            else:\n",
        "                print(f\"Skipped creating document for URL {url}\")"
      ],
      "metadata": {
        "id": "pjkIlkWErKlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Example usage with real data (replace with your actual data)\n",
        "    sheet_id = userdata.get('sheet_id')  # Replace with your actual Google Sheet ID\n",
        "    sheet_names = userdata.get('sheet_names')  # Replace with the actual sheet names you want to process\n",
        "    user_email = userdata.get('email_address')  # Replace with the email you want to share the document with\n",
        "\n",
        "    # Replace this with your desired parent folder ID (from your provided Google Drive folder link)\n",
        "    parent_folder_id = '1ORlrqMM9wK3SUJUdIOB4yDq7p71_JL1z'  # This is the folder where \"Episode 8\" will be created\n",
        "\n",
        "    # Call the main process to extract URLs, scrape content, and export each to an individual Google Doc\n",
        "    process_urls_and_export_to_docs(sheet_id, sheet_names, user_email, parent_folder_id)"
      ],
      "metadata": {
        "id": "GCNUg8HlrLrD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccd64347-b168-4111-b5c1-798b39f3c9c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder 'Episode 20' created with ID: 1F-fqo6eMgkzK_EAKR7LsNSeTSd47Bvbv\n",
            "Folder 'Original' created with ID: 1HaxLGZ7LoSlBEw1d9F0OBc08OdJNUQBt\n",
            "Processing sheet: Top_picks\n",
            "Available sheet names: ['Historical finalist', 'Top_picks', 'AI and marketing', 'AI and marketing scoring', 'AI and marketing filtered', 'AI and media', 'AI and media scoring', 'AI and media filtered', 'AI and innovation', 'AI and innovation scoring', 'AI and innovation filtered', 'AI and business', 'AI and business scoring', 'AI and business filtered', 'AI and management', 'AI and management scoring', 'AI and management filtered']\n",
            "Processing URL 1: https://www.healthcareitnews.com/news/himsscast-how-genai-can-reinvent-work-clinicians\n",
            "Document Original- 1 - 20250106 created and shared: https://docs.google.com/document/d/1mUsk3WHshPTrk13uh4o1C2snvfO_4FA3g1K3Bh-KFCw/edit\n",
            "Processing URL 2: https://www.ibtimes.com/green-stays-digital-keys-virtual-tours-future-accommodation-now-3757565\n",
            "Failed to retrieve content from https://www.ibtimes.com/green-stays-digital-keys-virtual-tours-future-accommodation-now-3757565: 403 Client Error: Forbidden for url: https://www.ibtimes.com/green-stays-digital-keys-virtual-tours-future-accommodation-now-3757565\n",
            "Skipped creating document for URL https://www.ibtimes.com/green-stays-digital-keys-virtual-tours-future-accommodation-now-3757565\n",
            "Processing URL 3: https://www.globenewswire.com/news-release/2024/12/31/3003195/0/en/BrandRep-Unveils-AI-Powered-Keyword-Generator-Transforming-SEO-for-Small-and-Mid-Sized-Businesses.html\n",
            "Document Original- 3 - 20250106 created and shared: https://docs.google.com/document/d/1XO0Lqz25oKs984D5ofZOSZPluixd1v7l9U2g6kzP0YU/edit\n",
            "Processing URL 4: https://www.pymnts.com/partnerships/2025/cerence-jaguar-land-rover-partner-ai-powered-in-car-experience/\n",
            "Document Original- 4 - 20250106 created and shared: https://docs.google.com/document/d/18-3unXeUXc2JprDWFUfG9_wYb-Jezw7aMGo4ZNhH1WI/edit\n",
            "Processing URL 5: https://www.pymnts.com/artificial-intelligence-2/2025/nvidia-google-and-the-hardware-revolution-behind-ais-rise-in-2024/\n",
            "Document Original- 5 - 20250106 created and shared: https://docs.google.com/document/d/1rkYhq846NM8MfXoD-nUfMEoRB62UEYh-hSHhj1DFVfo/edit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Old Scripts"
      ],
      "metadata": {
        "id": "3AHlgWzClG_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def create_google_doc(content, title):\n",
        "#     docs_service = build('docs', 'v1', credentials=creds)\n",
        "\n",
        "#     # Create a new Google Doc\n",
        "#     doc = docs_service.documents().create(body={\"title\": title}).execute()\n",
        "#     document_id = doc['documentId']\n",
        "\n",
        "#     # Add content to the Google Doc\n",
        "#     requests_body = [\n",
        "#         {\n",
        "#             'insertText': {\n",
        "#                 'location': {\n",
        "#                     'index': 1,\n",
        "#                 },\n",
        "#                 'text': content\n",
        "#             }\n",
        "#         }\n",
        "#     ]\n",
        "\n",
        "#     docs_service.documents().batchUpdate(documentId=document_id, body={'requests': requests_body}).execute()\n",
        "\n",
        "#     return document_id"
      ],
      "metadata": {
        "id": "eeJ2txzhq2UW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}